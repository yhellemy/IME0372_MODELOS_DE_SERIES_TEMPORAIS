<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Aula 9 - Funções de Autocovariância e Autocorrelação. Estimadores da Média Amostral e Autocorrelação</title>
    <meta charset="utf-8" />
    <meta name="author" content="Renato Rodrigues Silva" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Aula 9 - Funções de Autocovariância e Autocorrelação. Estimadores da Média Amostral e Autocorrelação
## Material baseado no livro Time Series Analysis: Univariate and Multivariate Methods (Willian W. S. Wei, 1990)
### Renato Rodrigues Silva
### Universidade Federal de Goiás.
### (updated: 2020-07-24)

---

#Funções de Autocovariância e de Autocorrelação - Introdução

- Uma série temporal estacionária é caracterizada pela média `\(\mu\)`,
variância `\(\sigma^2\)`, autocorrelações `\(\rho_k\)`, e autocorrelações
parciais `\(\phi_{kk}.\)`

- Os exatos valores destes parâmetros podem ser calculados se o *ensemble* de todas realizações fossem possível.

- De outra forma, eles podem ser estimados se multiplas realizações
independentes estejam disponíveis.

- Na maioria das aplicações, entretanto, é difícil ou impossível de obter múltiplas observações.

- Na prática, observamos apenas uma trajetória. Entretanto, para processos estacionários e ergódicos temos uma alternativa de substituir a média do ensemble pela média calculada ao longo do tempo.

- Ergodicidade significa que a média do ensemble é igual à média calculada ao longo do tempo [https://en.wikipedia.org/wiki/Ergodic_process].

      
      
      
---
#Funções de Autocovariância e de Autocorrelação

##Pressuposições

- Vamos assumir que o processo  `\(\left\{ Z_t, t \in \mathcal{Z} \right\}\)` seja estacionário de segunda ordem 

    - `\(E[Z_t) = \mu\)`; (média constante)
    - `\(E[Z^2_t] &lt; \infty, \phantom{11} \forall t \in \mathbf{Z}\)`; 
    - `\(Cov[Z_{t}, Z_{s}]\)` é uma função de `\(|t - s|\)`.

Lembre-se se o processo for Gaussiano e  estacionário de segunda ordem, ele é estritamente estacionário.

---
class: middle

##Função de Autocovariância    
  
`\begin{equation}
\gamma_k = Cov[Z_t, Z_{t+k}] = E[[Z_t - \mu][ Z_{t+k} - \mu]],
\end{equation}`

##Função de Autocorrelação  
  
`\begin{equation}
\rho_k = \frac{Cov[Z_t, Z_{t+k}]}{\sqrt{Var[Z_t]}\sqrt{Var[Z_{t+k}]}} = \frac{\gamma_k}{\gamma_0}
\end{equation}`

Note que `\(Var[Z_t] = Var[Z_{t+k}] = \gamma_0\)`, por pressuposição.

---
# Propriedades de Autocovariância e Autocorrelação

a.  `\(\gamma_0 = Var[Z_t]; \rho_0 = 1\)`

`\begin{equation}
\rho_0 = \frac{\gamma_0 }{\gamma_0 } = 
\frac{Var[Z_t]}{Var[Z_t]} = 1
\end{equation}`


b.  `\(|\gamma_k| \leq \gamma_0; |\rho_k| \leq 1\)`

`\begin{align}
|\rho_k| &amp;\leq 1 \Rightarrow -1 \leq \rho_k \leq 1 \Rightarrow 
-1 \leq \frac{\gamma_k}{\gamma_0} \leq 1 \Rightarrow -\gamma_0 \leq \gamma_k \leq \gamma_0
\end{align}`

c. `\(\gamma_k = \gamma_{-k}\)` e `\(\rho_k = \rho_{-k}\)` `\(\forall \phantom{i} k\)`, ou seja, `\(Cov[Z_t, Z_{t+k}] = Cov[Z_t, Z_{t-k}].\)`

---
## Propriedades de Autocovariância e Autocorrelação

d.  Função de autocorrelação e autocovariância são positivas semidefinidas no sentido de que

`\begin{align}
\sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j \gamma|t_i-t_j| \geq 0;
\sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j \rho|t_i-t_j| \geq 0;
\end{align}`

para qualquer conjunto `\(t_1, t_2, ..., t_n\)` e qualquer número real `\(\alpha_1, \ldots, \alpha_n.\)`

## Revisão:  Formas Quadráticas 

Seja `\(\mathbf{A}\)` for uma matriz simétrica e `\(\mathbf{y}\)` um vetor,
o produto 

`\begin{equation}
\mathbf{y}^{'}\mathbf{A}\mathbf{y} = \sum_{i=1}^n\sum_{j=1}^n a_{ij}y_i y_j
\end{equation}`

Se `\(\mathbf{y^{'}Ay} \geq 0\)` `\(\forall \phantom{1} \mathbf{y}\)` e existe pelo menos um `\(\mathbf{y} \neq \mathbf{0}\)` tal que `\(\mathbf{y^{'}Ay} = 0\)`, então  `\(\mathbf{y^{'}Ay}\)` é dito positiva semidefinida.

---
## Propriedades de Autocovariância e Autocorrelação

Definindo `\(X = \sum_{i=1}^n \alpha_i Z_{t_i}\)` segue


`\begin{align}
0 \leq Var[X] = \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j 
Cov[Z_{t_i}, Z_{t_j}] = \sum_{i=1}^n \sum_{j=1}^n 
\alpha_i \alpha_j \gamma |t_{i}-t_{j}|
\end{align}`

Prova:

`\begin{align}
Var[X] =&amp; Var\left[ \sum_{i=1}^n \alpha_i Z_{t_i} \right] \nonumber \\
=&amp; \sum_{i=1}^n \alpha_i^2 Var[Z_{t_i}] + 2 \sum_{i &lt; j} Cov[Z_{t_i},Z_{t_j} ] \nonumber \\
=&amp; \sum_{i=1}^n \sum_{j=1}^n  \alpha_i \alpha_j Cov[Z_{t_i},Z_{t_j} ] \nonumber \\
=&amp; \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j \gamma |t_{i}-t_{j}|
\end{align}`


---
class: middle
## Propriedades de Autocovariância e Autocorrelação

Assim é importante saber que nem toda função que satisfazem as propriedades (a) à (c) pode ser 
autocovariância ou autocorrelação  para um processo.

Uma condição necessária para uma função ser um autocovariância é a função ser positiva semidefinida.

A prova desse fato pode ser vista [aqui](https://www.math-stat.unibe.ch/e237483/e237655/e243381/e281679/files281692/Chap13_ger.pdf).

---
class: middle, inverse, center
#Média Amostral

---
class: middle
##Média Amostral

Com apenas uma única realização, um estimador natural para média `\(\mu = E[Z_t]\)` de um processo estacionário é o estimador amostral
`\begin{equation}
\bar{Z} = \frac{1}{n}\sum_{t=1}^n Z_t
\end{equation}`

Pode-se mostrar que o valor esperado do estimador dá-se por:
`\begin{align}
E[\bar{Z}] = E[\frac{1}{n}\sum_{t=1}^n Z_t] = \frac{1}{n}\sum_{t=1}^n E[Z_t] = \frac{1}{n}\sum_{t=1}^n \mu = \mu
\end{align}`

E a variância é dada por:

`\begin{align}
Var[\bar{Z}] =&amp; \frac{1}{n^2}\sum_{t=1}^n \sum_{s=1}^nCov[Z_t, Z_s] \\
=&amp; \frac{\gamma_0}{n^2} \sum_{k=-(n-1)}^{n-1}(n - |k|)\rho_k = \frac{\gamma_0}{n} \sum_{k=-(n-1)}^{n-1}\left(1 - \frac{|k|}{n}\right)\rho_k.
\end{align}`



---
class: middle
##Média Amostral


Assim, se `\(\lim_{n \rightarrow \infty} \frac{\gamma_0}{n} \sum_{k= -(n-1)}^{n-1}\left(1 - \frac{|k|}{n}\right)\rho_k\)` for finito, então `\(Var(\bar{Z}) \rightarrow 0\)`, quando `\(n \rightarrow \infty\)` e `\(\bar{Z}\)` é um estimador consistente em média quadrática para `\(\mu\)` [consistência](https://bookdown.org/egarpor/inference/point-est.html#point-est-consistent), [consistência 2](https://en.wikipedia.org/wiki/Consistent_estimator#:~:text=In%20statistics%2C%20a%20consistent%20estimator,in%20probability%20to%20%CE%B80.)

###Definição:

Uma sequência de estimadores `\(\left\{ \theta_n: n \in \mathbb{N} \right\}\)` é consistente em média quadrática para `\(\theta\)` se

`\(\lim_{n \rightarrow \infty} B\left[\hat{\theta}_n\right] = 0\)` e
`\(\lim_{n \rightarrow \infty} Var[\hat{\theta}_n] = 0.\)`

Revisão de leis dos grandes números podem ser vistas [aqui](https://pt.wikipedia.org/wiki/Lei_dos_grandes_n%C3%BAmeros), [aqui](https://en.wikipedia.org/wiki/Law_of_large_numbers#Differences_between_the_weak_law_and_the_strong_law).

Vídeo interessante [aqui](https://www.youtube.com/watch?v=Bn0wWZENeQI).
Codigo no [R](https://rstudio-pubs-static.s3.amazonaws.com/202144_71c017d6692c47d885f452b41a89f100.html.)


---
## Função Autocovariância Amostral

`\begin{equation}
\hat{\gamma}_k = \frac{1}{n}\sum_{i=1}^{n-k}(Z_t - \bar{Z})(Z_{t+k}-\bar{Z})
\end{equation}`

Pode-se mostrar que 

`\begin{equation}
E[\hat{\gamma}_k] \approx \gamma_k -  \frac{k}{n}\gamma_k - \left( \frac{n-k}{n}\right) Var(\bar{Z})
\end{equation}`
é assintoticamente não viesado. Ou seja, para `\(n \rightarrow \infty\)`, `\(E[\hat{\gamma}_k] \rightarrow \gamma_k\)`. 

Quando o processo for Gaussiano, pode-se demonstrar que 

`\begin{equation}
Cov[\hat{\gamma}_k, \hat{\gamma}_{k+j}] \approx \frac{1}{n}\sum_{i=-\infty}^\infty(\gamma_i\gamma_{i+j} + \gamma_{i+k+j}\gamma_{i-k})
\end{equation}`
e
`\begin{equation}
Var[\hat{\gamma}_k] \approx \frac{1}{n}\sum_{i=-\infty}^\infty(\gamma_i^2 + \gamma_{i+k}\gamma_{i-k})
\end{equation}`




---
class: middle
## Função de Autocorrelação Amostral

`\begin{equation}
\hat{\rho}_k = \frac{\hat{\gamma}_k}{\hat{\gamma}_0} = \frac{\sum_{t=1}^{n-k}(Z_t - \bar{Z})(Z_{t+k} - \bar{Z})}{\sum_{t=1}^{n}(Z_t - \bar{Z})^2}
\end{equation}`

Para `\(n\)` suficientemente grande, `\(\hat{\rho}_k\)` tem uma distribuição aproximadamente normal com média `\(\rho_k\)` e variância

`\begin{equation}
Var[\hat{\rho}_k] \approx \frac{1}{n}\sum_{i=-\infty}^{\infty}(\rho_i^2 + \rho_{i+k}\rho_{i-k} - 4\rho_k\rho_i\rho_{i-k} +  2\rho_k^2\rho_i^2)
\end{equation}`

Para processos nos quais `\(\rho_k = 0\)`, para `\(k &gt; m\)`, uma boa aproximação seria:

`\begin{equation}
Var[\hat{\rho}_k] \approx \frac{1}{n}(1 + 2\rho_1^2 + 2\rho_{2}^2 + \ldots + 2\rho_m^2)
\end{equation}`

---
class: middle
## Função de Autocorrelação Amostral

Na prática, estimaremos `\(Var[\hat{\rho}_k]\)` por meio de

`\begin{equation}
S_{\rho_k} = \sqrt{\frac{1}{n}(1 + 2\hat{\rho}_1^2 + \ldots + 2\hat{\rho}_m^2)}
\end{equation}`

---
class: middle
## Como Estimar a Autocorrelação Amostral



```r
library(tidyverse)
library(tsibble)
data("AirPassengers") 

dataset = AirPassengers %&gt;% as_tsibble() 
head(dataset)
```

```
## # A tsibble: 6 x 2 [1M]
##      index value
##      &lt;mth&gt; &lt;dbl&gt;
## 1 1949 Jan   112
## 2 1949 Feb   118
## 3 1949 Mar   132
## 4 1949 Apr   129
## 5 1949 May   121
## 6 1949 Jun   135
```


---
class: middle
## Como Estimar a Autocorrelação Amostral


```r
#Dados originais (top 10)
dataset$value[1:10]
```

```
##  [1] 112 118 132 129 121 135 148 148 136 119
```

```r
#Dados defasados (lag =1 )
lag(dataset$value, 1)[1:10]
```

```
##  [1]  NA 112 118 132 129 121 135 148 148 136
```

```r
#Dados defasados (lag =2 )
lag(dataset$value, 2)[1:10]
```

```
##  [1]  NA  NA 112 118 132 129 121 135 148 148
```

---
class: middle
## Como Estimar a Autocorrelação Amostral


```r
#Correlação entre Z_t e Z_{t-1} 
cor(dataset$value,lag(dataset$value, 1),use="pairwise.complete.obs")
```

```
## [1] 0.9601946
```

```r
#Correlação entre Z_t e Z_{t-1} 
cor(dataset$value[-1],dataset$value[-144])
```

```
## [1] 0.9601946
```

```r
#Use função acf do R
acf(dataset$value, type="correlation", lag.max =1 ,plot = FALSE)
```

```
## 
## Autocorrelations of series 'dataset$value', by lag
## 
##     0     1 
## 1.000 0.948
```


---
## Diferenças entre cor e acf

`$$\hat{\rho}_k = \frac{\sum_{t=1}^{n-k} y_t y_{t+k}}{\sum_{t=1}^n y_t^2}$$`
em que `\(y_t = z_t - \bar{z}\)`.


```r
#Como o R está calculando
yt = (dataset$value - mean(dataset$value))

sum(lag(yt,1)*yt, na.rm=T) / sum(yt^2)
```

```
## [1] 0.9480473
```

```r
#Use função acf do R
acf(dataset$value, type="correlation", lag.max =1 ,plot = FALSE)
```

```
## 
## Autocorrelations of series 'dataset$value', by lag
## 
##     0     1 
## 1.000 0.948
```


---
class: middle
## Diferenças entre cor e acf

Observe que a diferença entre os resultados da função `cor` e `acf` não é só o fator `\((\frac{n-1}{n})\)`, como está explicado nesse website [datacamp](https://www.datacamp.com/community/tutorials/autocorrelation-r). 

Detalhes adicionais podem ser encontrados nesse link [stackflow](https://stats.stackexchange.com/questions/81754/understanding-this-acf-output).

---
class: middle
## Apenas Mais um Exemplo - Autocorrelação com lag = 2


```r
cor(dataset$value,lag(dataset$value, 2),use="pairwise.complete.obs")
```

```
## [1] 0.8956753
```

```r
acf(dataset$value, type="correlation", lag.max =2 ,plot = FALSE)
```

```
## 
## Autocorrelations of series 'dataset$value', by lag
## 
##     0     1     2 
## 1.000 0.948 0.876
```

```r
#Como o R está calculando
yt = (dataset$value - mean(dataset$value))

sum(lag(yt,2)*yt, na.rm=T) / sum(yt^2)
```

```
## [1] 0.8755748
```
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
